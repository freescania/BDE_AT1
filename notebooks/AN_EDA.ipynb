{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on NYC Taxi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local spark session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('nyc-taxi-data') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters \n",
    "bucket_name = \"nyc-tlc\" # s3 bucket name with required nyc tlc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to read S3 bucket\n",
    "def list_bucket_contents(bucket, match=''):\n",
    "    files = []\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket_resource = s3_resource.Bucket(bucket)\n",
    "    for key in bucket_resource.objects.all():\n",
    "        if match in key.key:\n",
    "            files.append(key.key)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read yellow and green taxi data for respective years\n",
    "colours = [\"yellow\",\"green\"]\n",
    "years = [\"2019\",\"2020\"]\n",
    "files = []\n",
    "\n",
    "for year in years:\n",
    "    for colour in colours:\n",
    "        match = colour + \"_tripdata_\" + year\n",
    "        files.extend(list_bucket_contents(bucket=bucket_name, match=match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trip data/yellow_tripdata_2019-01.csv',\n",
       " 'trip data/yellow_tripdata_2019-02.csv',\n",
       " 'trip data/yellow_tripdata_2019-03.csv',\n",
       " 'trip data/yellow_tripdata_2019-04.csv',\n",
       " 'trip data/yellow_tripdata_2019-05.csv',\n",
       " 'trip data/yellow_tripdata_2019-06.csv',\n",
       " 'trip data/yellow_tripdata_2019-07.csv',\n",
       " 'trip data/yellow_tripdata_2019-08.csv',\n",
       " 'trip data/yellow_tripdata_2019-09.csv',\n",
       " 'trip data/yellow_tripdata_2019-10.csv',\n",
       " 'trip data/yellow_tripdata_2019-11.csv',\n",
       " 'trip data/yellow_tripdata_2019-12.csv',\n",
       " 'trip data/green_tripdata_2019-01.csv',\n",
       " 'trip data/green_tripdata_2019-02.csv',\n",
       " 'trip data/green_tripdata_2019-03.csv',\n",
       " 'trip data/green_tripdata_2019-04.csv',\n",
       " 'trip data/green_tripdata_2019-05.csv',\n",
       " 'trip data/green_tripdata_2019-06.csv',\n",
       " 'trip data/green_tripdata_2019-07.csv',\n",
       " 'trip data/green_tripdata_2019-08.csv',\n",
       " 'trip data/green_tripdata_2019-09.csv',\n",
       " 'trip data/green_tripdata_2019-10.csv',\n",
       " 'trip data/green_tripdata_2019-11.csv',\n",
       " 'trip data/green_tripdata_2019-12.csv',\n",
       " 'trip data/yellow_tripdata_2020-01.csv',\n",
       " 'trip data/yellow_tripdata_2020-02.csv',\n",
       " 'trip data/yellow_tripdata_2020-03.csv',\n",
       " 'trip data/yellow_tripdata_2020-04.csv',\n",
       " 'trip data/yellow_tripdata_2020-05.csv',\n",
       " 'trip data/yellow_tripdata_2020-06.csv',\n",
       " 'trip data/yellow_tripdata_2020-07.csv',\n",
       " 'trip data/yellow_tripdata_2020-08.csv',\n",
       " 'trip data/yellow_tripdata_2020-09.csv',\n",
       " 'trip data/yellow_tripdata_2020-10.csv',\n",
       " 'trip data/yellow_tripdata_2020-11.csv',\n",
       " 'trip data/yellow_tripdata_2020-12.csv',\n",
       " 'trip data/green_tripdata_2020-01.csv',\n",
       " 'trip data/green_tripdata_2020-02.csv',\n",
       " 'trip data/green_tripdata_2020-03.csv',\n",
       " 'trip data/green_tripdata_2020-04.csv',\n",
       " 'trip data/green_tripdata_2020-05.csv',\n",
       " 'trip data/green_tripdata_2020-06.csv',\n",
       " 'trip data/green_tripdata_2020-07.csv',\n",
       " 'trip data/green_tripdata_2020-08.csv',\n",
       " 'trip data/green_tripdata_2020-09.csv',\n",
       " 'trip data/green_tripdata_2020-10.csv',\n",
       " 'trip data/green_tripdata_2020-11.csv',\n",
       " 'trip data/green_tripdata_2020-12.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read January 2019 yellow taxi cab data from S3 bucket\n",
    "yellow_df = spark.read.csv(f\"s3a://{bucket_name}/trip data/yellow_tripdata_2020-07.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2020-07-01 00:25:32|  2020-07-01 00:33:39|              1|         1.50|         1|                 N|         238|          75|           2|          8|  0.5|    0.5|         0|           0|                  0.3|         9.3|                   0|\n",
      "|       1| 2020-07-01 00:03:19|  2020-07-01 00:25:43|              1|         9.50|         1|                 N|         138|         216|           1|       26.5|  0.5|    0.5|         0|           0|                  0.3|        27.8|                   0|\n",
      "|       2| 2020-07-01 00:15:11|  2020-07-01 00:29:24|              1|         5.85|         1|                 N|         230|          88|           2|       18.5|  0.5|    0.5|         0|           0|                  0.3|        22.3|                 2.5|\n",
      "|       2| 2020-07-01 00:30:49|  2020-07-01 00:38:26|              1|         1.90|         1|                 N|          88|         232|           1|          8|  0.5|    0.5|      2.36|           0|                  0.3|       14.16|                 2.5|\n",
      "|       2| 2020-07-01 00:31:26|  2020-07-01 00:38:02|              1|         1.25|         1|                 N|          37|          17|           2|        6.5|  0.5|    0.5|         0|           0|                  0.3|         7.8|                   0|\n",
      "|       1| 2020-07-01 00:09:00|  2020-07-01 00:34:39|              1|         9.70|         1|                 N|         140|          61|           1|         30|    3|    0.5|         0|           0|                  0.3|        33.8|                 2.5|\n",
      "|       2| 2020-07-01 00:44:08|  2020-07-01 00:58:12|              1|         5.27|         1|                 N|         137|         260|           1|       16.5|  0.5|    0.5|      6.09|           0|                  0.3|       26.39|                 2.5|\n",
      "|       2| 2020-07-01 00:49:20|  2020-07-01 00:56:44|              1|         1.32|         1|                 N|         166|          41|           2|        7.5|  0.5|    0.5|         0|           0|                  0.3|         8.8|                   0|\n",
      "|       2| 2020-07-01 00:21:59|  2020-07-01 00:25:12|              1|          .73|         1|                 N|         239|         142|           1|          5|  0.5|    0.5|      1.32|           0|                  0.3|       10.12|                 2.5|\n",
      "|       2| 2020-07-01 00:08:28|  2020-07-01 00:36:18|              1|        18.65|         2|                 N|         132|         249|           1|         52|    0|    0.5|     11.06|           0|                  0.3|       66.36|                 2.5|\n",
      "|       1| 2020-07-01 00:26:44|  2020-07-01 00:43:46|              2|         8.00|         1|                 N|         138|         112|           1|         24|  0.5|    0.5|         3|           0|                  0.3|        28.3|                   0|\n",
      "|       2| 2020-07-01 00:40:49|  2020-07-01 00:51:59|              3|         4.97|         1|                 N|          79|         195|           2|         16|  0.5|    0.5|         0|           0|                  0.3|        19.8|                 2.5|\n",
      "|       2| 2020-07-01 00:03:34|  2020-07-01 00:03:42|              1|          .00|         2|                 N|          45|          45|           1|         52|    0|    0.5|     11.06|           0|                  0.3|       66.36|                 2.5|\n",
      "|       2| 2020-07-01 00:08:53|  2020-07-01 00:12:42|              1|          .57|         1|                 N|         263|         263|           2|        4.5|  0.5|    0.5|         0|           0|                  0.3|         8.3|                 2.5|\n",
      "|       2| 2020-07-01 00:16:31|  2020-07-01 00:16:41|              1|          .00|         1|                 N|         263|         263|           1|        2.5|  0.5|    0.5|      1.89|           0|                  0.3|        8.19|                 2.5|\n",
      "|       2| 2020-07-01 00:36:43|  2020-07-01 01:02:48|              1|         9.41|         1|                 N|         170|         116|           1|       29.5|  0.5|    0.5|         3|           0|                  0.3|        36.3|                 2.5|\n",
      "|       1| 2020-07-01 00:16:31|  2020-07-01 00:16:43|              1|         2.80|         1|                 Y|         141|         141|           2|        2.5|  2.5|    0.5|         0|           0|                  0.3|         5.8|                 2.5|\n",
      "|       1| 2020-07-01 00:33:37|  2020-07-01 00:55:26|              2|        13.50|         1|                 Y|         137|         254|           2|       36.5|  2.5|    0.5|         0|           0|                  0.3|        39.8|                 2.5|\n",
      "|       2| 2020-07-01 00:15:15|  2020-07-01 00:17:44|              1|          .48|         1|                 N|         140|         140|           1|          4|  0.5|    0.5|      1.56|           0|                  0.3|        9.36|                 2.5|\n",
      "|       2| 2020-07-01 00:38:24|  2020-07-01 00:46:57|              1|         1.67|         1|                 N|         238|          75|           1|          8|  0.5|    0.5|      2.79|           0|                  0.3|       12.09|                   0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show first twenty rows of the imported file\n",
    "yellow_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema of data frame to show field data types and nullability\n",
    "yellow_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read January 2019 green taxi cab data from S3 bucket\n",
    "green_df = spark.read.csv(f\"s3a://{bucket_name}/trip data/green_tripdata_2019-01.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2018-12-21 15:17:29|  2018-12-21 15:18:57|                 N|         1|         264|         264|              5|          .00|          3|  0.5|    0.5|         0|           0|     null|                  0.3|         4.3|           2|        1|                null|\n",
      "|       2| 2019-01-01 00:10:16|  2019-01-01 00:16:32|                 N|         1|          97|          49|              2|          .86|          6|  0.5|    0.5|         0|           0|     null|                  0.3|         7.3|           2|        1|                null|\n",
      "|       2| 2019-01-01 00:27:11|  2019-01-01 00:31:38|                 N|         1|          49|         189|              2|          .66|        4.5|  0.5|    0.5|         0|           0|     null|                  0.3|         5.8|           1|        1|                null|\n",
      "|       2| 2019-01-01 00:46:20|  2019-01-01 01:04:54|                 N|         1|         189|          17|              2|         2.68|       13.5|  0.5|    0.5|      2.96|           0|     null|                  0.3|       19.71|           1|        1|                null|\n",
      "|       2| 2019-01-01 00:19:06|  2019-01-01 00:39:43|                 N|         1|          82|         258|              1|         4.53|         18|  0.5|    0.5|         0|           0|     null|                  0.3|        19.3|           2|        1|                null|\n",
      "|       2| 2019-01-01 00:12:35|  2019-01-01 00:19:09|                 N|         1|          49|          17|              1|         1.05|        6.5|  0.5|    0.5|         0|           0|     null|                  0.3|         7.8|           1|        1|                null|\n",
      "|       2| 2019-01-01 00:47:55|  2019-01-01 01:00:01|                 N|         1|         255|          33|              1|         3.77|       13.5|  0.5|    0.5|         0|           0|     null|                  0.3|        14.8|           1|        1|                null|\n",
      "|       1| 2019-01-01 00:12:47|  2019-01-01 00:30:50|                 N|         1|          76|         225|              1|         4.10|         16|  0.5|    0.5|         0|           0|     null|                  0.3|        17.3|           1|        1|                null|\n",
      "|       2| 2019-01-01 00:16:23|  2019-01-01 00:39:46|                 N|         1|          25|          89|              1|         7.75|       25.5|  0.5|    0.5|         0|           0|     null|                  0.3|        26.8|           1|        1|                null|\n",
      "|       2| 2019-01-01 00:58:02|  2019-01-01 01:19:02|                 N|         1|          85|          39|              1|         3.68|       15.5|  0.5|    0.5|         0|           0|     null|                  0.3|        16.8|           1|        1|                null|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "green_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: string (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      " |-- congestion_surcharge: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema of data frame to show field data types and nullability\n",
    "green_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended data type changes\n",
    "\n",
    "All fields imported as a string. The following data type conversions are required:\n",
    "\n",
    "* VendorID: string -> categorical (drop - needs processing intensive one hot encoding)\n",
    "* lpep_pickup_datetime: string -> timestamp\n",
    "* lpep_dropoff_datetime: string -> timestamp\n",
    "* store_and_fwd_flag: string -> integer (drop)\n",
    "* RatecodeID: string -> integer\n",
    "* PULocationID: string -> categorical (drop - needs processing intensive one hot encoding)\n",
    "* DOLocationID: string -> categorical (drop - needs processing intensive one hot encoding)\n",
    "* passenger_count: string -> integer\n",
    "* trip_distance: string -> double\n",
    "* fare_amount: string -> double\n",
    "* extra: string -> double\n",
    "* mta_tax: string -> double \n",
    "* tip_amount: string -> double\n",
    "* tolls_amount: string -> double\n",
    "* ehail_fee: string -> double \n",
    "* improvement_surcharge: string -> double \n",
    "* total_amount: string -> double\n",
    "* payment_type: string -> integer\n",
    "* trip_type: string -> integer (drop - is not in other data set)\n",
    "* congestion_surcharge: string -> double (drop - is not in other data set and should be included in extras per dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------+\n",
      "|    pickup_datetime|   dropoff_datetime|trip_duration|\n",
      "+-------------------+-------------------+-------------+\n",
      "|2020-07-13 03:14:35|2020-07-01 08:44:59|     -1016976|\n",
      "|2020-07-18 20:11:06|2019-07-18 20:47:07|    -31620239|\n",
      "|2020-07-29 17:07:55|2020-07-29 17:07:30|          -25|\n",
      "|2020-07-27 17:07:50|2020-07-27 17:07:35|          -15|\n",
      "|2020-07-30 18:07:57|2020-07-30 18:07:12|          -45|\n",
      "|2020-07-30 00:07:54|2020-07-30 00:07:03|          -51|\n",
      "+-------------------+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine if there are any drop offs before pickups\n",
    "yellow_df.withColumn(\"pickup_datetime\", F.unix_timestamp(F.col(\"tpep_pickup_datetime\"), \"yyyy-MM-dd' 'HH:mm:ss\").cast(\"timestamp\")).\\\n",
    "    withColumn(\"dropoff_datetime\", F.unix_timestamp(F.col(\"tpep_dropoff_datetime\"), \"yyyy-MM-dd' 'HH:mm:ss\").cast(\"timestamp\")).\\\n",
    "    withColumn(\"trip_duration\", (F.col(\"dropoff_datetime\").cast(\"long\") - F.col(\"pickup_datetime\").cast(\"long\"))).\\\n",
    "    filter(F.col(\"trip_duration\") < 0).\\\n",
    "    select([\"pickup_datetime\",\"dropoff_datetime\",\"trip_duration\"]).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------------------+-------------------+-------------+-----------+\n",
      "|tpep_pickup_datetime|    pickup_datetime|tpep_dropoff_datetime|   dropoff_datetime|trip_duration|fare_amount|\n",
      "+--------------------+-------------------+---------------------+-------------------+-------------+-----------+\n",
      "| 2020-07-01 00:03:34|2020-07-01 00:03:34|  2020-07-01 00:03:42|2020-07-01 00:03:42|            8|         52|\n",
      "| 2020-07-01 00:16:31|2020-07-01 00:16:31|  2020-07-01 00:16:41|2020-07-01 00:16:41|           10|        2.5|\n",
      "| 2020-07-01 00:16:31|2020-07-01 00:16:31|  2020-07-01 00:16:43|2020-07-01 00:16:43|           12|        2.5|\n",
      "| 2020-07-01 00:50:08|2020-07-01 00:50:08|  2020-07-01 00:50:18|2020-07-01 00:50:18|           10|         52|\n",
      "| 2020-07-01 00:09:00|2020-07-01 00:09:00|  2020-07-01 00:09:22|2020-07-01 00:09:22|           22|        2.5|\n",
      "| 2020-07-01 00:51:31|2020-07-01 00:51:31|  2020-07-01 00:51:39|2020-07-01 00:51:39|            8|        2.5|\n",
      "| 2020-07-01 01:38:25|2020-07-01 01:38:25|  2020-07-01 01:38:46|2020-07-01 01:38:46|           21|       39.2|\n",
      "| 2020-07-01 01:17:33|2020-07-01 01:17:33|  2020-07-01 01:17:33|2020-07-01 01:17:33|            0|         16|\n",
      "| 2020-07-01 01:53:47|2020-07-01 01:53:47|  2020-07-01 01:54:00|2020-07-01 01:54:00|           13|        2.5|\n",
      "| 2020-07-01 01:07:43|2020-07-01 01:07:43|  2020-07-01 01:07:48|2020-07-01 01:07:48|            5|        2.5|\n",
      "| 2020-07-01 01:13:53|2020-07-01 01:13:53|  2020-07-01 01:14:10|2020-07-01 01:14:10|           17|        2.5|\n",
      "| 2020-07-01 01:31:04|2020-07-01 01:31:04|  2020-07-01 01:31:20|2020-07-01 01:31:20|           16|         29|\n",
      "| 2020-07-01 03:00:56|2020-07-01 03:00:56|  2020-07-01 03:01:24|2020-07-01 03:01:24|           28|         20|\n",
      "| 2020-07-01 04:47:46|2020-07-01 04:47:46|  2020-07-01 04:48:13|2020-07-01 04:48:13|           27|       0.01|\n",
      "| 2020-07-01 04:49:04|2020-07-01 04:49:04|  2020-07-01 04:49:04|2020-07-01 04:49:04|            0|          0|\n",
      "| 2020-07-01 04:09:31|2020-07-01 04:09:31|  2020-07-01 04:09:44|2020-07-01 04:09:44|           13|        2.5|\n",
      "| 2020-07-01 05:59:07|2020-07-01 05:59:07|  2020-07-01 05:59:21|2020-07-01 05:59:21|           14|        2.5|\n",
      "| 2020-07-01 06:55:52|2020-07-01 06:55:52|  2020-07-01 06:56:01|2020-07-01 06:56:01|            9|        2.5|\n",
      "| 2020-07-01 06:27:21|2020-07-01 06:27:21|  2020-07-01 06:27:25|2020-07-01 06:27:25|            4|         52|\n",
      "| 2020-07-01 06:50:32|2020-07-01 06:50:32|  2020-07-01 06:50:40|2020-07-01 06:50:40|            8|         20|\n",
      "+--------------------+-------------------+---------------------+-------------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellow_df.withColumn(\"pickup_datetime\", F.unix_timestamp(F.col(\"tpep_pickup_datetime\"), \"yyyy-MM-dd' 'HH:mm:ss\").cast(\"timestamp\")).\\\n",
    "    withColumn(\"dropoff_datetime\", F.unix_timestamp(F.col(\"tpep_dropoff_datetime\"), \"yyyy-MM-dd' 'HH:mm:ss\").cast(\"timestamp\")).\\\n",
    "    withColumn(\"trip_duration\", (F.col(\"dropoff_datetime\").cast(\"long\") - F.col(\"pickup_datetime\").cast(\"long\"))).\\\n",
    "    filter(F.col(\"trip_duration\") < 30).\\\n",
    "    select([\"tpep_pickup_datetime\",\"pickup_datetime\",\"tpep_dropoff_datetime\",\"dropoff_datetime\",\"trip_duration\",\"fare_amount\"]).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at payment types 6 = voided trips\n",
    "yellow_df.filter(F.col(\"payment_type\").astype(IntegerType()) == 6).\\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3312"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at payment types 4 = dispute\n",
    "yellow_df.filter(F.col(\"payment_type\").astype(IntegerType()) == 4).\\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at payment types misclassified above 6\n",
    "yellow_df.filter(F.col(\"payment_type\").astype(IntegerType()) > 6).\\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at RatecodeID misclassified above 6\n",
    "yellow_df.filter(F.col(\"RatecodeID\").astype(IntegerType()) > 6).\\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8988"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at mta tax consistently $0.50\n",
    "yellow_df.filter(F.col(\"mta_tax\").astype(DoubleType()) != 0.50).\\\n",
    "    count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19506"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for less than one passenger\n",
    "yellow_df.filter(F.col(\"passenger_count\").astype(IntegerType()) < 1).\\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for tip amount less than zero\n",
    "yellow_df.filter(F.col(\"tip_amount\").astype(DoubleType()) < 0).\\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for tolls_amount less than zero\n",
    "yellow_df.filter(F.col(\"tolls_amount\").astype(DoubleType()) < 0).\\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4470"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for improvement_surcharge consistently $0.30\n",
    "yellow_df.filter(F.col(\"improvement_surcharge\").astype(DoubleType()) != 0.30).\\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3710"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for fare_amount less than zero\n",
    "yellow_df.filter(F.col(\"fare_amount\").astype(DoubleType()) < 0).\\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1416"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for extra greater than $1 (congestion or night time fee)\n",
    "yellow_df.filter(F.col(\"extra\").astype(DoubleType()) > 3.5).\\\n",
    "    count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look for trip_distance less than zero\n",
    "yellow_df.filter(F.col(\"trip_distance\").astype(DoubleType()) < 0).\\\n",
    "    count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`ehail_fee`' given input columns: [passenger_count, tolls_amount, VendorID, congestion_surcharge, PULocationID, DOLocationID, total_amount, tpep_dropoff_datetime, payment_type, tip_amount, tpep_pickup_datetime, extra, mta_tax, store_and_fwd_flag, fare_amount, RatecodeID, trip_distance, improvement_surcharge];;\\n'Filter (cast('ehail_fee as double) >= 0)\\n+- Relation[VendorID#10,tpep_pickup_datetime#11,tpep_dropoff_datetime#12,passenger_count#13,trip_distance#14,RatecodeID#15,store_and_fwd_flag#16,PULocationID#17,DOLocationID#18,payment_type#19,fare_amount#20,extra#21,mta_tax#22,tip_amount#23,tolls_amount#24,improvement_surcharge#25,total_amount#26,congestion_surcharge#27] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o34.filter.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`ehail_fee`' given input columns: [passenger_count, tolls_amount, VendorID, congestion_surcharge, PULocationID, DOLocationID, total_amount, tpep_dropoff_datetime, payment_type, tip_amount, tpep_pickup_datetime, extra, mta_tax, store_and_fwd_flag, fare_amount, RatecodeID, trip_distance, improvement_surcharge];;\n'Filter (cast('ehail_fee as double) >= 0)\n+- Relation[VendorID#10,tpep_pickup_datetime#11,tpep_dropoff_datetime#12,passenger_count#13,trip_distance#14,RatecodeID#15,store_and_fwd_flag#16,PULocationID#17,DOLocationID#18,payment_type#19,fare_amount#20,extra#21,mta_tax#22,tip_amount#23,tolls_amount#24,improvement_surcharge#25,total_amount#26,congestion_surcharge#27] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3130fc51bd61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Look for trip_distance less than zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0myellow_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ehail_fee\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoubleType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"condition should be string or Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`ehail_fee`' given input columns: [passenger_count, tolls_amount, VendorID, congestion_surcharge, PULocationID, DOLocationID, total_amount, tpep_dropoff_datetime, payment_type, tip_amount, tpep_pickup_datetime, extra, mta_tax, store_and_fwd_flag, fare_amount, RatecodeID, trip_distance, improvement_surcharge];;\\n'Filter (cast('ehail_fee as double) >= 0)\\n+- Relation[VendorID#10,tpep_pickup_datetime#11,tpep_dropoff_datetime#12,passenger_count#13,trip_distance#14,RatecodeID#15,store_and_fwd_flag#16,PULocationID#17,DOLocationID#18,payment_type#19,fare_amount#20,extra#21,mta_tax#22,tip_amount#23,tolls_amount#24,improvement_surcharge#25,total_amount#26,congestion_surcharge#27] csv\\n\""
     ]
    }
   ],
   "source": [
    "# Look for trip_distance less than zero\n",
    "yellow_df.filter(F.col(\"ehail_fee\").astype(DoubleType()) >= 0).\\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
